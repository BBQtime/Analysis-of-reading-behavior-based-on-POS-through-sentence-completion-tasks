# Analysis-of-reading-behavior-based-on-POS-through-sentence-completion-tasks

# Abstract
In this paper, we evaluate the reaction of people’s behavior towards information
from reading incomplete sentence based on their eye movements’ metrics while they
try to fill the missing word.The experiment has potential meaning for psycholinguistics as well, however, this paper only uncovers the relationship between part of
speech(POS) and the missing word in one sentence, or how syntactic works with incomplete grammar dependency of POS. We discovered that fixation metrics perform
differently under missing word scenario with ordinary reading tasks. Through an
eye-tracking experiment, the eye tracker obtained eye measurements from 19 subjects in 2 groups of missing word sentences of different POS. In the first group of
the stimuli, there was 30 individual Noun missing sentences, and in the other group,
30 individual Verb missing sentences are placed. Gaze metrics of each AOIs(tokens
of a sentence) were obtained over 19 subjects, a qualitative analysis was conducted
afterward based on the mean of total fixation duration on AOIs.
![alt text](https://github.com/insinuate/Analysis-of-reading-behavior-based-on-POS-through-sentence-completion-tasks/blob/master/experiment_heatmap.png?raw=true)

# Stimuli and experiment setup
For stimuli content, We constructed 60 different sentences, each with a missing word.
Out of the 60, half of it was concerning only Nouns as missing words, and the other one
Verbs. We paid attention not to put too many PPs, as ambiguity is not something with
which we wanted to deal. The sentence length differs. There are some 5-word sentences,
those are the shortest ones, and the longest one was 15 word long. They are all one
sentences, except for a few, where it was necessary for the context to have two, but they
are closely related. Some of the sentences had emotion words in them, mostly because
we wanted to see how subjectivity or personality changes the gaze duration on these
specific words.

For the construction part, we plant a notification page at the beginning of the experiment, briefing the upcoming experiment sections. Participants could press any key
to start the experiment. For the stimuli sections, we put each sentence in one page, and
form them with two parts, Noun and Verb, thus each part has 30 pages, in each page,
3an enlarged sentence is placed in the center of the page. We also manually increased
space between each word in the sentence for the convenience of later AOI region selection process. Between each section, we planted a notification screen with highly colored
contrast information towards stimuli, indicating the break between each section and the
beginning of next section. Users could press any key to swap to next page of sentence or
section. While they reading the sentence, a voice recorder is placed to record their voice.
Participants are required to speak out the word which they’d like to fill in the missing
place, this is also informed by conductor and consent form. After the last sentence of
stimuli, a black page of notification was showed to the participant, notifying the ending
of the experiment, "Click to finish and please notify the conductor before your leave...".


The voice recording is a method to testify to guarantee that participant is focusing
on the task, which is also an authentication for later selection of gaze data and has
no functionality towards data collection and analysis. List of our experiment stimuli
sentences could be seen at App



